{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# quitely pip install  necessary libraries\n",
        "!pip install -q osmnx\n",
        "!pip install -q mapclassify\n",
        "!pip install -q beautifulsoup4"
      ],
      "metadata": {
        "id": "tfW0mKNtDGLd"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "CTxCQS6X5chc"
      },
      "outputs": [],
      "source": [
        "\"\"\"takes address as input to prompt and\n",
        "converts it to coordinates and a geodataframe\n",
        "author: rayne davidson\"\"\"\n",
        "\n",
        "\n",
        "from pandas.core.dtypes.cast import invalidate_string_dtypes\n",
        "import pandas as pd\n",
        "import osmnx as ox\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Point\n",
        "import matplotlib as mpl\n",
        "import folium\n",
        "import matplotlib.pyplot as plt\n",
        "from mapclassify import classify\n",
        "import numpy as np\n",
        "import string\n",
        "\n",
        "# mapping lowercase cardinal directions (in the event user enters as lowercase)\n",
        "# to a dictionary of lowercase to uppercase cardinal directions. this only needs\n",
        "# to be done once as a global list so I dont think it needs to be in a function\n",
        "directions_list = [\"n\", \"ne\", \"nw\", \"s\", \"se\", \"sw\", \"e\", \"w\"]\n",
        "upper_directions = [x.upper() for x in directions_list]\n",
        "DIRECTIONS_DICT = dict(zip(directions_list, upper_directions))\n",
        "\n",
        "\n",
        "def user_input(prompt):\n",
        "  \"\"\"asks user for given prompt\"\"\"\n",
        "  return input(prompt)\n",
        "\n",
        "\n",
        "def user_address_input():\n",
        "  \"\"\"asks user for address. does not move address forward until address is\n",
        "  confirmed by user\"\"\"\n",
        "  prompt = \"Address for evaluation: \"\n",
        "  address = format_address(user_input(prompt))\n",
        "  while confirmation(address) == False:\n",
        "    address = re_entry()\n",
        "  return address\n",
        "\n",
        "\n",
        "def user_confirm(address):\n",
        "  \"\"\"asks user to confirm given address (in event of typo). address is formatted\n",
        "  when given back to user to look nicer\"\"\"\n",
        "  confirm_prompt = f'Please confirm that the address {address} is correct (Y/N): '\n",
        "  confirmation = user_input(confirm_prompt)\n",
        "  return confirmation\n",
        "\n",
        "\n",
        "def format_address(address):\n",
        "  \"\"\"formats address to capitalize cardinal directions and first letter\n",
        "  of road names - chat gpt helped with this\"\"\"\n",
        "  address_individuals = address.split()\n",
        "  formatted_words = []\n",
        "  for word in address_individuals:\n",
        "    if word.lower() in DIRECTIONS_DICT:\n",
        "      formatted_words.append(DIRECTIONS_DICT[word.lower()])\n",
        "    else:\n",
        "      formatted_words.append(word.capitalize())\n",
        "  formatted_address = ' '.join(formatted_words)\n",
        "  return formatted_address\n",
        "\n",
        "\n",
        "def confirmation(address):\n",
        "  \"\"\"checks if user confirms address or if address should be re-entered.\n",
        "  returns true or false\"\"\"\n",
        "  valid_confirmations = [\"yes\", \"y\", \"Yes\", \"Y\"]\n",
        "  confirmation = user_confirm(address)\n",
        "  if confirmation in valid_confirmations:\n",
        "    return True\n",
        "  elif confirmation not in valid_confirmations:\n",
        "    return False\n",
        "\n",
        "\n",
        "def re_entry():\n",
        "  \"\"\"prompts user to re-enter address if confirmation is denied\"\"\"\n",
        "  reenter_prompt = \"Please re-enter address: \"\n",
        "  address = format_address(user_input(reenter_prompt))\n",
        "  return address\n",
        "\n",
        "\n",
        "def geocode_address(address):\n",
        "  \"\"\"geocodes address so it can be explored in gpd\"\"\"\n",
        "  try:\n",
        "    geocode_result = ox.geocode(address)\n",
        "    return geocode_result\n",
        "  except:\n",
        "    return None\n",
        "\n",
        "\n",
        "def valid_geocode(geocode_result):\n",
        "  \"\"\"returns Point geometry for a valid geocoded address\"\"\"\n",
        "  y, x = geocode_result\n",
        "  point = Point(x, y)  # Create a Point geometry\n",
        "  return point\n",
        "\n",
        "\n",
        "def address_to_gdf(address, point):\n",
        "  \"\"\"creates geodataframe from geocoded address\"\"\"\n",
        "  pr_crs = 4326\n",
        "  gdf = gpd.GeoDataFrame({'Address': [address]}, geometry=[point], crs=pr_crs)\n",
        "  return gdf\n",
        "\n",
        "\n",
        "def invalid_geocode(geocode_result):\n",
        "  \"\"\"if address is not able to be geocoded, prompts to try again\n",
        "  until address is successfully geocoded\"\"\"\n",
        "  while geocode_result is None:\n",
        "      print(\"No valid address found. Please try again\")\n",
        "      print(\"Try entering just street address (no city/state)\")\n",
        "      prompt = \"Please re-enter address: \"\n",
        "      address = format_address(user_input(prompt))\n",
        "      while confirmation(address) == False:\n",
        "        address = re_entry()\n",
        "      geocode_result = geocode_address(address)\n",
        "  point = valid_geocode(geocode_result)\n",
        "  print(\"New address is successfully validated!\")\n",
        "  return point\n",
        "\n",
        "\n",
        "def geocode(address):\n",
        "  \"\"\"applies geocoding to address. Moves to invalid geocode re-entry prompt\n",
        "  if address is not able to be geocoded\"\"\"\n",
        "  geocode_result = geocode_address(address)\n",
        "  if geocode_result is not None:\n",
        "    point = valid_geocode(geocode_result)\n",
        "    print(\"Address is successfully validated!\")\n",
        "    return point\n",
        "  elif geocode_result is None:\n",
        "    point = invalid_geocode(geocode_result)\n",
        "    return point\n",
        "\n",
        "\n",
        "def confirmed_address():\n",
        "  \"\"\"moving on with valid address and global coordinates for versatility\"\"\"\n",
        "  global ADDRESS\n",
        "  ADDRESS = user_address_input()\n",
        "  point = geocode(ADDRESS)\n",
        "  address_gdf = address_to_gdf(ADDRESS, point)\n",
        "  return address_gdf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download data (Web scraping)"
      ],
      "metadata": {
        "id": "F9N1INKJFTnr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"web scraping for tsunami, liquifaction, volcano and geology data\n",
        "from the WA state Dept of Natural Resources (DNR)\"\"\"\n",
        "\n",
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "def set_soup(url_link):\n",
        "  url = url_link\n",
        "  response = requests.get(url)\n",
        "\n",
        "  if response.status_code == 200:\n",
        "      soup = BeautifulSoup(response.text, 'html.parser')\n",
        "      return soup\n",
        "  else:\n",
        "      print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
        "      exit()\n",
        "\n",
        "def soup_find(hazard):\n",
        "  \"\"\"finds soup links of a peticular hazard\"\"\"\n",
        "  links = soup.find_all('a', string=hazard)\n",
        "  return links\n",
        "\n",
        "def get_url_stuff(link):\n",
        "  \"\"\"downloads stuff from the link(s) to pwd\"\"\"\n",
        "  absolute_url = urljoin(url, link['href'])\n",
        "  filename = os.path.basename(absolute_url)\n",
        "  file_response = requests.get(absolute_url)\n",
        "  if file_response.status_code == 200:\n",
        "      with open(filename, 'wb') as file:\n",
        "          file.write(file_response.content)\n",
        "      print(f\"Downloaded {filename}\")\n",
        "  else:\n",
        "      print(f\"Failed to download {filename}. Status code: {file_response.status_code}\")\n",
        "\n",
        "def download_data(url_link, hazard):\n",
        "  \"\"\"downloads relevant data from the relevant place\"\"\"\n",
        "  soup = set_soup(url_link)\n",
        "  links = soup_find(hazard)\n",
        "  for link in links:\n",
        "    get_url_stuff(link)\n",
        "\n",
        "def download_haz_data():\n",
        "  \"\"\"downloads hazard data from specific place (WA DNR)\"\"\"\n",
        "  url_link = 'https://www.dnr.wa.gov/programs-and-services/geology/publications-and-data/gis-data-and-databases'\n",
        "\n",
        "  # all of these are the link names on the DNR website for the relevant data I need\n",
        "  hazards_lst = [\"Tsunami Hazard\", \"Ground Response\", \"Simplified Volcanic Hazards\", \"1:500,000 scale\"]\n",
        "  for hazard in hazards_lst:\n",
        "    download_data(url_link, hazard)\n",
        "\n",
        "# download_haz_data() # i intetionally have this turned off because i dont need to download it again"
      ],
      "metadata": {
        "id": "8udIgc7K-CKP"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the web scraping wont actually be very useful for you, as I had to run this and open the files in ArcGIS Pro to save them as shapefiles that can be used in geopandas. The only pythoning way around this was using arcpy which requires an ArcGIS Pro subscription to be logged in/function, which i wasn't sure how well that would work when submitted/used elsewhere, so I opted for doing it this way\n",
        "\n",
        "\n",
        "**you will need to upload the shapefile data before you can proceed**"
      ],
      "metadata": {
        "id": "vI7nq2qbhm31"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source: https://www.dnr.wa.gov/programs-and-services/geology/publications-and-data/gis-data-and-databases"
      ],
      "metadata": {
        "id": "ANoH_KzwQltB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read in the Hazards Data"
      ],
      "metadata": {
        "id": "iSUeGUnMDb_7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I attempted to make a GitHub repository that the data could be pulled from but had a lot of issues making it work (including file sizes too large)"
      ],
      "metadata": {
        "id": "ITMDGACuzB12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tsunami = gpd.read_file(\"TsunamiHazardAreas.shp\")\n",
        "liquif = gpd.read_file(\"liquifaction_susceptibility.shp\")\n",
        "volcano = gpd.read_file(\"volcano.shp\")\n",
        "geology = gpd.read_file(\"geology.shp\")"
      ],
      "metadata": {
        "id": "poyPZUVYJKEy"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data description dictionaries"
      ],
      "metadata": {
        "id": "RZk4-TzXLZ3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"i could have left at least most of the descriptions that came with\n",
        "the data, but the reason i am making this project at all is because the data\n",
        "from the goverment can be difficult to obtain and/or understand, so this\n",
        "was my attempt at making the hazards more understandable for normal people\"\"\"\n",
        "\n",
        "# TSUNAMI\n",
        "# ------------------------------------\n",
        "# descriptions derrived from hazard.TSUNAMI_SC.unique() list, but put in more user-friendly readability (simplified)\n",
        "tsunami_dict = (\n",
        "    {\n",
        "      tsunami.TSUNAMI_SC.unique()[0]\n",
        "      :\"Possible tsunami inundation in 9.0 Cascadia Fault Earthquake\",\n",
        "      tsunami.TSUNAMI_SC.unique()[1]\n",
        "      :\"Possible tsunami inundation in 7.3 Tacoma Fault Earthquake\",\n",
        "      tsunami.TSUNAMI_SC.unique()[2]\n",
        "      :\"Possible tsunami inundation in 7.3 Tacoma Fault Earthquake\",\n",
        "      tsunami.TSUNAMI_SC.unique()[3]\n",
        "      :\"Possible tsunami inundation in 9.0 Cascadia Fault Earthquake\",\n",
        "      tsunami.TSUNAMI_SC.unique()[4]\n",
        "      :\"Possible tsunami inundation in 7.5 Cascadia Fault Earthquake\",\n",
        "      tsunami.TSUNAMI_SC.unique()[5]\n",
        "      :\"Possible tsunami inundation in 9.1 Cascadia Fault Earthquake\",\n",
        "      tsunami.TSUNAMI_SC.unique()[6]\n",
        "      :\"Possible tsunami inundation in 9.1 Cascadia Fault Earthquake\",\n",
        "      tsunami.TSUNAMI_SC.unique()[7]\n",
        "      :\"Possible tsunami inundation - unspecified magitude or fault\",\n",
        "      tsunami.TSUNAMI_SC.unique()[8]\n",
        "      :\"Possible tsunami inundation in 9.1 Cascadia Fault Earthquake\",\n",
        "      tsunami.TSUNAMI_SC.unique()[9]\n",
        "      :\"Possible coastal, estuarine and waterway inundation; not analyzed\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# LIQUIFACTION\n",
        "# ------------------------------------\n",
        "# descriptions derrived from liquif.LIQUEFAC_1.unique() list, but put in more user-friendly readability (simplified)\n",
        "liquif_dict = (\n",
        "    {\n",
        "      liquif.LIQUEFAC_1.unique()[0]:\"No risk of liquefaction\",\n",
        "      liquif.LIQUEFAC_1.unique()[1]:\"High risk of liquefaction\",\n",
        "      liquif.LIQUEFAC_1.unique()[2]:\"No risk of liquefaction\",\n",
        "      liquif.LIQUEFAC_1.unique()[3]:\"Low risk of liquefaction\",\n",
        "      liquif.LIQUEFAC_1.unique()[4]:\"Low to Moderate risk of liquefaction\",\n",
        "      liquif.LIQUEFAC_1.unique()[5]:\"Moderate risk of liquefaction\",\n",
        "      liquif.LIQUEFAC_1.unique()[6]:\"Moderate to High risk of liquefaction\",\n",
        "      liquif.LIQUEFAC_1.unique()[7]:\"No risk of liquefaction\",\n",
        "      liquif.LIQUEFAC_1.unique()[8]:\"Very Low risk of liquefaction\",\n",
        "      liquif.LIQUEFAC_1.unique()[9]:\"Very Low to Low risk of liquefaction\",\n",
        "      liquif.LIQUEFAC_1.unique()[10]:\"No risk of liquefaction\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# VOLCANO\n",
        "# ------------------------------------\n",
        "# descriptions derrived from volcano.HAZARD_TYP.unique() list, but put in more user-friendly readability (simplified)\n",
        "volcano_risk_dict = (\n",
        "    {\n",
        "      volcano.HAZARD_TYP.unique()[0]:\"Lahar risk zone\",\n",
        "      volcano.HAZARD_TYP.unique()[1]:\"Near-volcano hazards zone\",\n",
        "      volcano.HAZARD_TYP.unique()[2]:\"Regional lava flow risk zone\",\n",
        "      volcano.HAZARD_TYP.unique()[3]:\"Tephra ash risk zone\",\n",
        "      volcano.HAZARD_TYP.unique()[4]:\"Lahar sediment risk zone\",\n",
        "    }\n",
        ")\n",
        "\n",
        "# the volcano data has a list of actions to do in the event of each hazard\n",
        "# i feel as though that information is important (not just knowing the hazard)\n",
        "v_haz = volcano.drop_duplicates(subset=[\"HAZARD_TYP\"])\n",
        "v_haz = v_haz.drop([\"VOLCANO\", \"DESCRIPTIO\", \"HYPERLINK\", \"DEFINITION\",\n",
        "                    \"Shape_Leng\", \"Shape_Area\", \"geometry\"], axis=1)\n",
        "\n",
        "haz_actions = v_haz.values.tolist()\n",
        "actions = [i[1:] for i in haz_actions]\n",
        "\n",
        "# descriptions derrived from volcano.HAZARD_TYP.unique() list, but put in more user-friendly readability (simplified)\n",
        "volcano_actions_dict = (\n",
        "    {\n",
        "      volcano.HAZARD_TYP.unique()[0]:[actions[0][0], actions[0][1],\n",
        "                                      actions[0][2], actions[0][3]],\n",
        "      volcano.HAZARD_TYP.unique()[1]:[actions[1][0], actions[1][1],\n",
        "                                      actions[1][2], actions[1][3]],\n",
        "      volcano.HAZARD_TYP.unique()[2]:[actions[2][0], actions[2][1],\n",
        "                                      actions[2][2], actions[2][3]],\n",
        "      volcano.HAZARD_TYP.unique()[3]:[actions[3][0], actions[3][1],\n",
        "                                      actions[3][2], actions[3][3]],\n",
        "      volcano.HAZARD_TYP.unique()[4]:[actions[4][0], actions[4][1],\n",
        "                                      actions[4][2], actions[4][3]],\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "# GEOLOGY (not a hazard just fun to know because i'm a geologist)\n",
        "# ------------------------------------\n",
        "# I did do this manually from the unit names given in the ArcGIS file because they only used labels inside arc to name,\n",
        "# rather than having an the name of the geologic units as actual data. yes it was a pain in the foot\n",
        "\n",
        "labels = []\n",
        "for i in geology.Label.unique():\n",
        "  labels.append(i)\n",
        "\n",
        "geo_dict = (\n",
        "    {\n",
        "      labels[0]:f\"Quaternary alluvium ({labels[0]})\",\n",
        "      labels[1]:f\"Tertiary volcanic rocks, Columbia River Basalt Group ({labels[1]})\",\n",
        "      labels[2]:f\"Tertiary volcanic rocks ({labels[2]})\",\n",
        "      labels[3]:f\"Quaternary–Tertiary volcanic rocks ({labels[3]})\",\n",
        "      labels[4]:f\"Pleistocene outburst-flood  deposits ({labels[4]})\",\n",
        "      labels[5]:f\"Quaternary–Tertiary continental sedimentary rocks and deposits ({labels[5]})\",\n",
        "      labels[6]:f\"Water ({labels[6]})\",\n",
        "      labels[7]:f\"Holocene dune sand ({labels[7]})\",\n",
        "      labels[8]:f\"Quaternary volcanic rocks ({labels[8]})\",\n",
        "      labels[9]:f\"Quaternary mass-wasting deposits ({labels[9]})\",\n",
        "      labels[10]:f\"Tertiary continental sedimentary rocks ({labels[10]})\",\n",
        "      labels[11]:f\"Tertiary fragmental volcanic rocks ({labels[11]})\",\n",
        "      labels[12]:f\"Tertiary intrusive rocks ({labels[12]})\",\n",
        "      labels[13]:f\"Quaternary loess ({labels[13]})\",\n",
        "      labels[14]:f\"Pleistocene alpine glacial drift ({labels[14]})\",\n",
        "      labels[15]:f\"Quaternary–Tertiary intrusive rocks ({labels[15]})\",\n",
        "      labels[16]:f\"Quaternary intrusive rocks ({labels[16]})\",\n",
        "      labels[17]:f\"Quaternary fragmental volcanic rocks and deposits (includes lahars) ({labels[17]})\",\n",
        "      labels[18]:f\"Mesozoic marine sedimentary rocks ({labels[18]})\",\n",
        "      labels[19]:f\"Mesozoic intrusive rocks ({labels[19]})\",\n",
        "      labels[20]:f\"Mesozoic metasedimentary and metavolcanic rocks ({labels[20]})\",\n",
        "      labels[21]:f\"Mesozoic metasedimentary rocks ({labels[21]})\",\n",
        "      labels[22]:f\"Mesozoic–Paleozoic metasedimentary and metavolcanic rocks ({labels[22]})\",\n",
        "      labels[23]:f\"Tertiary nearshore sedimentary rocks ({labels[23]})\",\n",
        "      labels[24]:f\"Ice ({labels[24]})\",\n",
        "      labels[25]:f\"Tertiary marine sedimentary rocks ({labels[25]})\",\n",
        "      labels[26]:f\"Tertiary volcanic rocks, Crescent Formation ({labels[26]})\",\n",
        "      labels[27]:f\"Mesozoic orthogneiss ({labels[27]})\",\n",
        "      labels[28]:f\"Mesozoic metavolcanic rocks ({labels[28]})\",\n",
        "      labels[29]:f\"Mesozoic volcanic rocks ({labels[29]})\",\n",
        "      labels[30]:f\"Pleistocene continental glacial drift ({labels[30]})\",\n",
        "      labels[31]:f\"Tectonic zones; areas of intense cataclasis, including mylonitization ({labels[31]})\",\n",
        "      labels[32]:f\"Mesozoic–Paleozoic heterogeneous metamorphic rocks ({labels[32]})\",\n",
        "      labels[33]:f\"Precambrian metasedimentary rocks ({labels[33]})\",\n",
        "      labels[34]:f\"Mesozoic–Paleozoic amphibolite ({labels[34]})\",\n",
        "      labels[35]:f\"Precambrian heterogeneous metamorphic rocks ({labels[35]})\",\n",
        "      labels[36]:f\"Paleozoic metavolcanic rocks ({labels[36]})\",\n",
        "      labels[37]:f\"Mesozoic–Paleozoic ultramafic rocks ({labels[37]})\",\n",
        "      labels[38]:f\"Mesozoic heterogeneous metamorphic rocks ({labels[38]})\",\n",
        "      labels[39]:f\"Mesozoic gneiss ({labels[39]})\",\n",
        "      labels[40]:f\"Tertiary–Cretaceous intrusive rocks ({labels[40]})\",\n",
        "      labels[41]:f\"Paleozoic–Precambrian metasedimentary rocks ({labels[41]})\",\n",
        "      labels[42]:f\"Mesozoic migmatite and mixed metamorphic and igneous rocks ({labels[42]})\",\n",
        "      labels[43]:f\"Paleozoic metasedimentary rocks ({labels[43]})\",\n",
        "      labels[44]:f\"Precambrian metavolcanic rocks ({labels[44]})\",\n",
        "      labels[45]:f\"Precambrian intrusive rocks ({labels[45]})\",\n",
        "      labels[46]:f\"Paleozoic intrusive rocks ({labels[46]})\",\n",
        "      labels[47]:f\"Tertiary–Cretaceous orthogneiss ({labels[47]})\",\n",
        "      labels[48]:f\"Paleozoic gneiss ({labels[48]})\",\n",
        "      labels[49]:f\"Mesozoic continental sedimentary rocks ({labels[49]})\",\n",
        "      labels[50]:f\"Mesozoic amphibolite ({labels[50]})\",\n",
        "      labels[51]:f\"Paleozoic metasedimentary and metavolcanic rocks ({labels[51]})\",\n",
        "      labels[52]:f\"Tertiary–Cretaceous gneiss ({labels[52]})\",\n",
        "      labels[53]:f\"Mesozoic nearshore sedimentary rocks ({labels[53]})\"\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "MMI0iVNbfO_F"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Creating 3 common columns in all datasets\n",
        "and assigning them the correct data\"\"\"\n",
        "\n",
        "# I tried to do this pythonically and it didn't work\n",
        "\n",
        "# def create_new_cols(gdf, gdf_dot_column, hazard_dict, haz_name, act_dict={}):\n",
        "#  \"\"\"creates 3 new columns on df so they have common cols\n",
        "#  and assigns their relevant data\"\"\"\n",
        "#   gdf[\"Risk\"] = gdf_dot_column.map(hazard_dict)\n",
        "#   if gdf != volcano:\n",
        "#     gdf[\"Action\"] = np.nan\n",
        "#     gdf[\"Hazard\"] = haz_name\n",
        "#   if gdf == volcano:\n",
        "#     gdf[\"Action\"] = gdf_dot_column.map(act_dict)\n",
        "#     gdf[\"Hazard\"] = haz_name\n",
        "#   return gdf\n",
        "\n",
        "# tsunami = create_new_cols(tsunami, tsunami.TSUNAMI_SC, tsunami_dict, \"Tsunami Risk\")\n",
        "# liquif = create_new_cols(liquif, liquif.LIQUEFAC_1, liquif_dict, \"Liquifaction Susceptibility\")\n",
        "# volcano = create_new_cols(volcano, volcano.HAZARD_TYP, volcano_risk_dict, \"Volcanic Risks\", volcano_action_dict)\n",
        "# geology = create_new_cols(geology, geology.Label, geo_dict, \"Local Geology (no risk)\")\n",
        "\n",
        "\n",
        "# ------------------------------------\n",
        "# I genuinely have no idea why this doesnt work in a function\n",
        "# so here is it manually done\n",
        "\n",
        "tsunami[\"Risk\"] = tsunami.TSUNAMI_SC.map(tsunami_dict)\n",
        "tsunami[\"Action\"] = np.nan\n",
        "tsunami[\"Hazard\"] = \"Tsunami Risk\"\n",
        "\n",
        "liquif[\"Risk\"] = liquif.LIQUEFAC_1.map(liquif_dict)\n",
        "liquif[\"Action\"] = np.nan\n",
        "liquif[\"Hazard\"] = \"Liquifaction Susceptibility\"\n",
        "\n",
        "volcano[\"Risk\"] = volcano.HAZARD_TYP.map(volcano_risk_dict)\n",
        "volcano[\"Action\"] = volcano.HAZARD_TYP.map(volcano_actions_dict)\n",
        "volcano[\"Hazard\"] = \"Volcanic Risks\"\n",
        "\n",
        "geology[\"Risk\"] = geology.Label.map(geo_dict)\n",
        "geology[\"Action\"] = np.nan\n",
        "geology[\"Hazard\"] = \"Local Geology (no risk)\""
      ],
      "metadata": {
        "id": "gvcmFIYvO_mp"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GDF processing"
      ],
      "metadata": {
        "id": "CWu9S9r_UCEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Proces the hazard data:\n",
        "- setting correct original coord reference system (crs)\n",
        "- reset crs to allign with the crs of the address\n",
        "- remove & rename all columns except the relevant ones\"\"\"\n",
        "\n",
        "from pyproj import CRS\n",
        "\n",
        "hazards_list = [\"Tsunami Risk\", \"Liquifaction Susceptibility\",\n",
        "                \"Volcanic Risks\", \"Local Geology (no risk)\"]\n",
        "\n",
        "\n",
        "def set_custom_crs(gdf):\n",
        "  \"\"\"sets custom crs\n",
        "  Information from metadata\"\"\"\n",
        "  custom_crs = CRS(\n",
        "    proj=\"lcc\", lat_1=47.33333333333334,\n",
        "    lat_2=45.83333333333334, lat_0=45.33333333333334,\n",
        "    lon_0=-120.5, x_0=500000,\n",
        "    y_0=0, datum=\"NAD83 HARN\",\n",
        "    units=\"us-ft\", ellps=\"GRS80\")\n",
        "\n",
        "  gdf = gdf.set_crs(custom_crs, allow_override=True)\n",
        "  gdf = gdf.to_crs(\"EPSG:4326\")\n",
        "  return gdf\n",
        "\n",
        "\n",
        "def relevant_columns(gdf):\n",
        "  \"\"\"keeps only the relevant columns\n",
        "  of each gdf in the desired order\"\"\"\n",
        "  gdf = gdf[[\"Hazard\", \"Risk\", \"Action\", \"geometry\"]]\n",
        "  return gdf\n",
        "\n",
        "\n",
        "def gdfs_crs(gdfs):\n",
        "  \"\"\"sets correct crs for all gdfs\"\"\"\n",
        "  crs_gdfs = []\n",
        "  for gdf in gdfs:\n",
        "    gdf2 = set_custom_crs(gdf)\n",
        "    crs_gdfs.append(gdf2)\n",
        "  return crs_gdfs\n",
        "\n",
        "\n",
        "def filter_gdfs(gdfs):\n",
        "  \"\"\"keep only the relevant columns of all gdfs\"\"\"\n",
        "  filtered_gdfs = []\n",
        "  crs_gdfs = gdfs_crs(gdfs)\n",
        "  for gdf in crs_gdfs:\n",
        "    gdf2 = relevant_columns(gdf)\n",
        "    filtered_gdfs.append(gdf2)\n",
        "  return filtered_gdfs\n",
        "\n",
        "\n",
        "def preprocess_gdfs():\n",
        "  \"\"\"pepares gdfs to needs\"\"\"\n",
        "  gdfs_list = [tsunami, liquif, volcano, geology]\n",
        "  gdfs = filter_gdfs(gdfs_list)\n",
        "  return gdfs"
      ],
      "metadata": {
        "id": "Zrcw05DFF1PY"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"find what risks are associates with the address\"\"\"\n",
        "\n",
        "def find_overlap(address_gdf):\n",
        "  \"\"\"find which hazards intersect the address fromm all gdfs\"\"\"\n",
        "  gdfs = preprocess_gdfs()\n",
        "  overlap = []\n",
        "  for gdf in gdfs:\n",
        "    overlap.extend(gdf[\n",
        "        gdf.intersects(address_gdf.unary_union)\n",
        "        ].to_records(index=False))\n",
        "  return overlap\n",
        "\n",
        "\n",
        "def new_gdf(address_gdf):\n",
        "  \"\"\"creates a new gdf from data, renames columns\"\"\"\n",
        "  overlap = find_overlap(address_gdf)\n",
        "  haz_at_address = gpd.GeoDataFrame.from_records(overlap)\n",
        "  haz_at_address =  haz_at_address.rename(\n",
        "      columns={0:\"Hazard\", 1: \"Risk\", 2:\"Action\", 3:\"geometry\"})\n",
        "  return haz_at_address"
      ],
      "metadata": {
        "id": "5zRqQkVRReL4"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"produce a printed report of the hazards found at the address\"\"\"\n",
        "\n",
        "# associates simple names with official names. honestly idk, it made sense at the time\n",
        "HAZARDS = {\"tsunami\":[\"Tsunami Risk\", \"tsunami inundation\"],\n",
        "           \"liquifaction\": [\"Liquifaction Susceptibility\", \"liquifaction\"],\n",
        "           \"volcano\": [\"Volcanic Risks\", \"volcanic activity\"]}\n",
        "\n",
        "\n",
        "def find_str_risk(gdf, hazard, x=0):\n",
        "  \"\"\"finds string of just the risk associated\"\"\"\n",
        "  return gdf[\"Risk\"][gdf[\"Hazard\"] == hazard].drop_duplicates().values[x]\n",
        "\n",
        "\n",
        "def print_actions(gdf, hazard, x):\n",
        "  \"\"\"prints the actions to perform for the risk\n",
        "  volcanic risk is the only one with action items\"\"\"\n",
        "  actions = gdf[\"Action\"][gdf[\"Hazard\"] == hazard].values[x]\n",
        "  print(f'Recommended actions in the event of an eruption are as follows: \\n')\n",
        "  i = 1\n",
        "  for action in actions:\n",
        "    print(f'  {i}: {action} \\n')\n",
        "    i += 1\n",
        "\n",
        "\n",
        "def risk_print_statements(counter, risk, gdf, hazard, x):\n",
        "  \"\"\"print statements for each risk\"\"\"\n",
        "  print(f\"Hazard {counter}: {risk.capitalize()}. \\n\")\n",
        "  print(f'The {risk} risk factor at this address is: {find_str_risk(gdf, hazard, x)} \\n')\n",
        "\n",
        "\n",
        "def print_risks(gdf, hazard, risk, risk_count):\n",
        "  \"\"\"prints the risk statements for each hazard, and actions\n",
        "  if applicable\"\"\"\n",
        "  global NO_HAZ\n",
        "  global COUNTER\n",
        "  COUNTER = risk_count\n",
        "  doubles = len(gdf[\"Risk\"][gdf[\"Hazard\"] == hazard].drop_duplicates())\n",
        "  if any(gdf[\"Hazard\"] == hazard):\n",
        "    NO_HAZ = False\n",
        "    for x in range(doubles):\n",
        "      if hazard != \"Volcanic Risks\":\n",
        "        risk_print_statements(COUNTER, risk, gdf, hazard, x)\n",
        "        COUNTER += 1\n",
        "      if hazard == \"Volcanic Risks\":\n",
        "        risk_print_statements(COUNTER, risk, gdf, hazard, x)\n",
        "        print_actions(gdf, hazard, x)\n",
        "        COUNTER += 1\n",
        "  else:\n",
        "    print(f\"There is no risk of {risk} at this address \\n\")\n",
        "    NO_HAZ = True\n",
        "\n",
        "\n",
        "def print_by_hazard(gdf, input_hazard, risk_count):\n",
        "  \"\"\"prints risk factor statements\n",
        "  hazard options: tsunami, liquifaction or volcano\"\"\"\n",
        "  try:\n",
        "    hazard = HAZARDS.get(input_hazard)[0] # The hazard name as it appears in the df\n",
        "    risk = HAZARDS.get(input_hazard)[1] # The risk as it will be written for the summary\n",
        "    print_risks(gdf, hazard, risk, risk_count)\n",
        "  except:\n",
        "    print(\"There is an error\")\n",
        "\n",
        "\n",
        "def print_all_hazards(gdf):\n",
        "  \"\"\"iterates through hazards and prints all releavnt to address\"\"\"\n",
        "  hazards = [\"tsunami\", \"liquifaction\", \"volcano\"]\n",
        "  print(f'There are {len(gdf[\"Risk\"].drop_duplicates())-1} hazards & risks found at {ADDRESS}: \\n') # -1 because geology is not a risk\n",
        "  risk_count = 1\n",
        "  for hazard in hazards:\n",
        "    print_by_hazard(gdf, hazard, risk_count)\n",
        "    if NO_HAZ == False:\n",
        "      risk_count = COUNTER\n",
        "\n",
        "\n",
        "def print_geology(gdf):\n",
        "  \"\"\"prints geology at address\"\"\"\n",
        "  geo = gdf[\"Risk\"][gdf[\"Hazard\"] == \"Local Geology (no risk)\"].values[0]\n",
        "  print(f'The geology of {ADDRESS} is: {geo}. \\n*Note: This is not a hazard or risk, but may be useful to know \\n')\n",
        "\n",
        "\n",
        "def print_hazards_main(address_gdf):\n",
        "  \"\"\"main execution of processing the gdfs and printing\n",
        "  the hazards associated with the address\"\"\"\n",
        "  gdf = new_gdf(address_gdf)\n",
        "  print_geology(gdf)\n",
        "  print_all_hazards(gdf)"
      ],
      "metadata": {
        "id": "a7taZXFcZEkx"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  \"\"\"takes a users address and returns\n",
        "  what hazards are documented at the address\"\"\"\n",
        "  address_gdf = confirmed_address()\n",
        "  print(\"\")\n",
        "  print_hazards_main(address_gdf)"
      ],
      "metadata": {
        "id": "nLPqjASu6WRJ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "id": "0t6PsXurLfr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample addresses:\n",
        "-\t1 of each: 3 E Park St, Bay Center, WA 98527, United States\n",
        "-\tMulti-Volcanic, no tsunami: 24000 Spirit Lake Hwy, Toutle, WA 98649, United States\n",
        "-\tMulti-Tsunami: 48.33348176083236, -124.66031049466368\n",
        "-\tInvalid geocode: 13427 446th ave se north bend, WA, 98045, United States\n",
        "-\tValid geocode: 13427 446th ave se\n",
        "-\tFeel free to try out any other Washington State addresses or coordinates. In some cases, such as the two geocode examples above, information beyond the street address can result in invalid geocoding. Removing city, zip, country usually seems to work, but coordinates always work too.\n",
        "\n"
      ],
      "metadata": {
        "id": "bexk_LiSW15V"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o9xFgGHPkHb0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
